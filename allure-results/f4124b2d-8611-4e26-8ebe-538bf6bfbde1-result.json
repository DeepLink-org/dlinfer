{"name": "test_pipeline_chat_pytorch_tp1[internlm_model_data/internlm2-chat-7b]", "status": "failed", "statusDetails": {"message": "pytest_assume.plugin.FailedAssumption: \n1 Failed Assumptions:\n\ntests/ci_utils/pipeline_chat.py:134: AssumptionFailure\n>>\twith assume:\nAssertionError: result:False, reason:urumqi,乌鲁木齐,乌市,xinjiang,新疆,uwumqi,Ürümqi doesn't exist in", "trace": "tp = <class 'pytest_assume.plugin.FailedAssumption'>, value = None, tb = None\n\n    def reraise(tp, value, tb=None):\n        try:\n            if value is None:\n                value = tp()\n            if value.__traceback__ is not tb:\n>               raise value.with_traceback(tb)\n\n/usr/local/python3.10.5/lib/python3.10/site-packages/six.py:718: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nconfig = {'log_path': '/data2/wugeshui/GitHub/ci_log/logs', 'model_path': '/data2/share_data', 'pytorch_chat_model': ['internlm.../llama-2-7b-chat-hf', 'mixtral_model_data/Mixtral-8x7B-Instruct-v0.1'], 'tp_config': {'Mixtral-8x7B-Instruct-v0.1': 2}}\ncases_info = {'chinese_poem_case': [{'给我一首中文诗，需要添加标点符号，请用中文回答Give me a Chinese poem in Chinese': [{'contain': ['，', '。', 'poem', 'p...lease introduce some delicious foods': [{'contain': ['urumqi', '乌鲁木齐', '乌市', 'xinjiang', '新疆', 'uwumqi', ...]}]}], ...}\nmodel_case = 'internlm_model_data/internlm2-chat-7b', type = 'pytorch', worker_id = ''\n\n    def assert_pipeline_chat_log(config,\n                                 cases_info,\n                                 model_case,\n                                 type,\n                                 worker_id: str = ''):\n        log_path = config.get('log_path')\n    \n        config_log = os.path.join(\n            log_path, '_'.join([\n                'pipeline', 'config', type, worker_id,\n                model_case.split('/')[1] + '.log'\n            ]))\n    \n        allure.attach.file(config_log, attachment_type=allure.attachment_type.TEXT)\n    \n        for case in cases_info.keys():\n            if ('coder' in model_case\n                    or 'CodeLlama' in model_case) and 'code' not in case:\n                continue\n            msg = 'result is empty, please check again'\n            result = False\n            with allure.step('case - ' + case):\n                pipeline_chat_log = os.path.join(\n                    log_path, '_'.join([\n                        'pipeline', 'chat', type, worker_id,\n                        model_case.split('/')[1], case + '.log'\n                    ]))\n    \n                allure.attach.file(pipeline_chat_log,\n                                   attachment_type=allure.attachment_type.TEXT)\n    \n                with open(pipeline_chat_log, 'r') as f:\n                    lines = f.readlines()\n    \n                    for line in lines:\n                        if 'result:False, reason:' in line:\n                            result = False\n                            msg = line\n                            break\n                        if 'result:True, reason:' in line and result is False:\n                            result = True\n                            msg = ''\n    \n                with assume:\n>                   assert result, msg\nE                   pytest_assume.plugin.FailedAssumption: \nE                   1 Failed Assumptions:\nE                   \nE                   tests/ci_utils/pipeline_chat.py:134: AssumptionFailure\nE                   >>\twith assume:\nE                   AssertionError: result:False, reason:urumqi,乌鲁木齐,乌市,xinjiang,新疆,uwumqi,Ürümqi doesn't exist in\n\ntests/ci_utils/pipeline_chat.py:135: FailedAssumption"}, "steps": [{"name": "case - identity", "status": "passed", "attachments": [{"source": "65f1920e-3abb-4b23-abd6-04316fa9e382-attachment.txt", "type": "text/plain"}], "start": 1722945161296, "stop": 1722945161525}, {"name": "case - memory_test", "status": "passed", "attachments": [{"source": "faee7952-9852-4ce9-9789-847a97d5ee4d-attachment.txt", "type": "text/plain"}], "start": 1722945161525, "stop": 1722945161528}, {"name": "case - chinese_poem_case", "status": "passed", "attachments": [{"source": "0cb8e2c3-5a49-4972-893f-4996da4b9ab3-attachment.txt", "type": "text/plain"}], "start": 1722945161528, "stop": 1722945161531}, {"name": "case - traditional_chinese_case", "status": "passed", "attachments": [{"source": "dd9afc05-366d-4110-bd8f-2287870afc6e-attachment.txt", "type": "text/plain"}], "start": 1722945161531, "stop": 1722945161534}, {"name": "case - code_testcase", "status": "passed", "attachments": [{"source": "ce64a25a-dcc9-4142-8fd3-b8057c28c214-attachment.txt", "type": "text/plain"}], "start": 1722945161534, "stop": 1722945161536}], "attachments": [{"source": "95c49c2c-4104-4a43-979a-738d5b814bde-attachment.txt", "type": "text/plain"}, {"name": "stdout", "source": "3f1d54d5-c82f-4da7-8292-4f22cb48e227-attachment.txt", "type": "text/plain"}, {"name": "stderr", "source": "b603debf-a96f-4bd6-a1f8-b1cd70b4e438-attachment.txt", "type": "text/plain"}], "parameters": [{"name": "model", "value": "'internlm_model_data/internlm2-chat-7b'"}], "start": 1722945028245, "stop": 1722945161537, "uuid": "1f580a31-0ead-4cdf-9b19-1e99ac1204a8", "historyId": "e372cfc38b7aa005d02bedf06424b6e9", "testCaseId": "d75a9809b4118a38ea4835d6768095cb", "fullName": "e2e.test_pipeline_chat_pytorch#test_pipeline_chat_pytorch_tp1", "labels": [{"name": "tag", "value": "flaky(reruns=0)"}, {"name": "tag", "value": "gpu_num_1"}, {"name": "tag", "value": "pipeline_chat_pytorch"}, {"name": "tag", "value": "@pytest.mark.usefixtures('common_case_config')"}, {"name": "tag", "value": "order(6)"}, {"name": "parentSuite", "value": "e2e"}, {"name": "suite", "value": "test_pipeline_chat_pytorch"}, {"name": "host", "value": "inferext_ci_card45_docker"}, {"name": "thread", "value": "1838759-MainThread"}, {"name": "framework", "value": "pytest"}, {"name": "language", "value": "cpython3"}, {"name": "package", "value": "e2e.test_pipeline_chat_pytorch"}]}